{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "#importing all the necessary libraries\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import nltk as nltk\n",
    "import re\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T06:35:17.314023Z",
     "start_time": "2024-09-05T06:35:16.326904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#setting the pytorch device to cuda\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")  # Print GPU name\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Training on CPU...\")"
   ],
   "id": "c76546494d7f9182",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T18:55:40.223377Z",
     "start_time": "2024-09-05T18:55:00.266957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#loading the english-german euro dataset\n",
    "CD = \"./Seq2Seq Model/DataSet/EN-DE/\"\n",
    "#source language is english\n",
    "SL = 'EN'\n",
    "#translated language is german\n",
    "TL = 'DE'\n",
    "df = pd.read_csv(\"D:\\projects\\Seq2Seq Model\\DataSet\\EN-DE\\EN-DE.txt\", sep = \"\\t\", header= None)[[0,1]].rename(columns = {0:SL, 1:TL})"
   ],
   "id": "3312b58f250eca19",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\thehe\\AppData\\Local\\Temp\\ipykernel_10192\\918787768.py:6: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  df = pd.read_csv(\"D:\\projects\\Seq2Seq Model\\DataSet\\EN-DE\\EN-DE.txt\", sep = \"\\t\", header= None)[[0,1]].rename(columns = {0:SL, 1:TL})\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T18:55:46.890681Z",
     "start_time": "2024-09-05T18:55:46.859682Z"
    }
   },
   "cell_type": "code",
   "source": "df.head(5693624)",
   "id": "9236f24c0a437f78",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                        EN  \\\n",
       "0                  Commission Regulation (EC) No 1788/2004   \n",
       "1                                       of 15 October 2004   \n",
       "2        fixing the minimum selling prices for butter f...   \n",
       "3              THE COMMISSION OF THE EUROPEAN COMMUNITIES,   \n",
       "4        Having regard to the Treaty establishing the E...   \n",
       "...                                                    ...   \n",
       "5693619                             Third country code [1]   \n",
       "5693620                              Standard import value   \n",
       "5693621  Country nomenclature as fixed by Commission Re...   \n",
       "5693622           Code ‘999’ stands for ‘of other origin’.   \n",
       "5693623  OJ L 337, 24.12.1994, p. 66. Regulation as las...   \n",
       "\n",
       "                                                        DE  \n",
       "0             Verordnung (EG) Nr. 1788/2004 der Kommission  \n",
       "1                                     vom 15. Oktober 2004  \n",
       "2        zur Festsetzung der Mindestverkaufspreise für ...  \n",
       "3         DIE KOMMISSION DER EUROPÄISCHEN GEMEINSCHAFTEN —  \n",
       "4        gestützt auf den Vertrag zur Gründung der Euro...  \n",
       "...                                                    ...  \n",
       "5693619                                 Drittland-Code [1]  \n",
       "5693620                            Pauschaler Einfuhrpreis  \n",
       "5693621  Nomenklatur der Länder gemäß der Verordnung (E...  \n",
       "5693622          Der Code „999“ steht für „Verschiedenes“.  \n",
       "5693623  ABl. L 337 vom 24.12.1994, S. 66. Verordnung z...  \n",
       "\n",
       "[5693624 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Commission Regulation (EC) No 1788/2004</td>\n",
       "      <td>Verordnung (EG) Nr. 1788/2004 der Kommission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of 15 October 2004</td>\n",
       "      <td>vom 15. Oktober 2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fixing the minimum selling prices for butter f...</td>\n",
       "      <td>zur Festsetzung der Mindestverkaufspreise für ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMMISSION OF THE EUROPEAN COMMUNITIES,</td>\n",
       "      <td>DIE KOMMISSION DER EUROPÄISCHEN GEMEINSCHAFTEN —</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Having regard to the Treaty establishing the E...</td>\n",
       "      <td>gestützt auf den Vertrag zur Gründung der Euro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5693619</th>\n",
       "      <td>Third country code [1]</td>\n",
       "      <td>Drittland-Code [1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5693620</th>\n",
       "      <td>Standard import value</td>\n",
       "      <td>Pauschaler Einfuhrpreis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5693621</th>\n",
       "      <td>Country nomenclature as fixed by Commission Re...</td>\n",
       "      <td>Nomenklatur der Länder gemäß der Verordnung (E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5693622</th>\n",
       "      <td>Code ‘999’ stands for ‘of other origin’.</td>\n",
       "      <td>Der Code „999“ steht für „Verschiedenes“.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5693623</th>\n",
       "      <td>OJ L 337, 24.12.1994, p. 66. Regulation as las...</td>\n",
       "      <td>ABl. L 337 vom 24.12.1994, S. 66. Verordnung z...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5693624 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#function that tokenises a given string and returns a list of individual words\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    # removes punctuations and split into words\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens"
   ],
   "id": "95a3ab8393d4816a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#building the english and german dictionaries \n",
    "english_words = []\n",
    "german_words = []\n",
    "error_count = 0 \n",
    "#before, we use the lists to store all the words inside the corpus\n",
    "for i in tqdm.tqdm(range(len(df))):\n",
    "    try:\n",
    "        #loading the english and german text strings for a certain row\n",
    "        english_text_array = df['EN'].values[i:i+1]\n",
    "        english_text = english_text_array[0]\n",
    "        german_text_array = df['DE'].values[i:i+1]\n",
    "        german_text = german_text_array[0]\n",
    "        \n",
    "        \n",
    "        #step to remove all the punctuations and non-words\n",
    "        english_text = tokenize(english_text)\n",
    "        german_text = tokenize(german_text)\n",
    "    except Exception as e:\n",
    "        error_count = error_count+1\n",
    "        continue\n",
    "    \n",
    "    #if the try block of code runs then we can push the words into their respective lists\n",
    "    english_words.extend(english_text)\n",
    "    german_words.extend(german_text)\n",
    "print(\"Number of times the pre-processing failed: \",error_count)\n",
    "print(\"Number of elements in german token list is: \",len(german_words))\n",
    "\n",
    "# building vocabulary and storing the most frequent words\n",
    "def build_vocab(tokens,max_size):\n",
    "    #Count the frequency of each item\n",
    "    counter = Counter(tokens)\n",
    "    # sort by frequency and then alphabetically \n",
    "    sorted_vocab = sorted(counter.items(), key= lambda x: (-x[1],x[0]))\n",
    "    # create dictionary mapping each token to a unique indx\n",
    "    # we leave 4 spaces for special token entries\n",
    "    vocab_words = {word: idx for idx, (word, _) in enumerate(sorted_vocab,start = 4)}\n",
    "    \n",
    "    extra_words = (len(vocab_words)-max_size)+4\n",
    "    for i in range(extra_words):\n",
    "        vocab_words.popitem()\n",
    "    \n",
    "    # special tokens\n",
    "    vocab_words['<PAD>'] = 0\n",
    "    vocab_words['<SOS>'] = 1\n",
    "    vocab_words['<EOS>'] = 2\n",
    "    vocab_words['<UNK>'] = 3 \n",
    "    return vocab_words\n",
    "\n",
    "\n",
    "english_dict = build_vocab(english_words,20000)\n",
    "german_dict = build_vocab(german_words,40000)\n",
    "#saving the dictionary \n",
    "with open(\"./dictionary/english_dict.pickle\", \"wb\") as file:\n",
    "    pickle.dump(english_dict,file)\n",
    "with open(\"./dictionary/german_dict.pickle\", \"wb\") as file:\n",
    "    pickle.dump(german_dict,file)"
   ],
   "id": "a5e21429025772ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T06:35:22.514431Z",
     "start_time": "2024-09-05T06:35:22.495085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#loading the dictionaries\n",
    "import pickle\n",
    "with open(\"./dictionary/english_dict.pickle\", \"rb\") as file:\n",
    "    english_dict = pickle.load(file)\n",
    "with open(\"./dictionary/german_dict.pickle\", \"rb\") as file:\n",
    "    german_dict = pickle.load(file)"
   ],
   "id": "5e0cf9f25edae329",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T06:35:25.085053Z",
     "start_time": "2024-09-05T06:35:25.080114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_size, embedding_size, hidden_size,num_layers, p):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size,embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size,hidden_size, num_layers, dropout=p)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        \n",
    "        return hidden, cell"
   ],
   "id": "4b6e1ca96a2b4da0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T06:35:25.543201Z",
     "start_time": "2024-09-05T06:35:25.538177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,input_size,embedding_size,hidden_size,output_size,num_layers,p):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_Size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self,x,hidden, cell):\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden,cell) = self.rnn(embedding,(hidden,cell))\n",
    "        \n",
    "        predictions = self.fc(outputs)\n",
    "        predictions = predictions.squeeze(0)\n",
    "        \n",
    "        return predictions, hidden, cell"
   ],
   "id": "e7e9826a8faab0f5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T06:35:25.984591Z",
     "start_time": "2024-09-05T06:35:25.979079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self,source,target, teacher_force_ratio = 0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        \n",
    "        target_vocab_len = len(english_dict)\n",
    "        \n",
    "        outputs = torch.zeros(target_len,batch_size,target_vocab_len).to(device)\n",
    "        \n",
    "        hidden, cell = self.encoder(source)\n",
    "        #grab start token\n",
    "        x = target[0]\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "            best_guess = output.argmax(1)\n",
    "            \n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "            \n",
    "        return outputs\n",
    "            \n"
   ],
   "id": "2bdd2e48ba9115d1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T06:35:26.588715Z",
     "start_time": "2024-09-05T06:35:26.580311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#dataset class and dataloader\n",
    "class english_german_translation(Dataset):\n",
    "    def __init__(self,english_dict,german_dict):\n",
    "        CD = \"./Seq2Seq Model/DataSet/EN-DE/\"\n",
    "        #source language is english\n",
    "        SL = 'EN'\n",
    "        #translated language is german\n",
    "        TL = 'DE'\n",
    "        df = pd.read_csv(\"D:\\projects\\Seq2Seq Model\\DataSet\\EN-DE\\EN-DE.txt\", sep = \"\\t\", header= None)[[0,1]].rename(columns = {0:SL, 1:TL})\n",
    "        self.data = df[\"DE\"].values\n",
    "        self.target = df[\"EN\"].values\n",
    "        \n",
    "        self.english_dict = english_dict\n",
    "        self.german_dict = german_dict\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.target[idx]\n",
    "        \n",
    "        x = self.string_to_tokens(x,self.german_dict,True)\n",
    "        y = self.string_to_tokens(y,self.english_dict,False)\n",
    "        \n",
    "        return x,y\n",
    "            \n",
    "    def string_to_tokens(self,string,dict,gate):\n",
    "        string = string.lower()\n",
    "        # removes punctuations and split into words\n",
    "        string = re.findall(r'\\b\\w+\\b', string)\n",
    "        indices = [dict.get(word,3) for word in string]\n",
    "        if gate == True:\n",
    "            indices = list(reversed(indices))\n",
    "        tensor = torch.tensor(indices, dtype=torch.long)\n",
    "        return tensor\n",
    "        \n",
    "def custom_collate_fn(batch):\n",
    "    # Unzip the batch of tuples (x, y)\n",
    "    source_sequences, target_sequences = zip(*batch)\n",
    "    \n",
    "    # Find the maximum lengths for padding\n",
    "    max_source_len = max(len(seq) for seq in source_sequences)\n",
    "    max_target_len = max(len(seq) for seq in target_sequences)\n",
    "    \n",
    "    # Initialize padded tensors with padding index (e.g., 0 or a custom padding index)\n",
    "    pad_index = 0  # Adjust this if you have a specific padding index\n",
    "    padded_sources = torch.full((len(batch), max_source_len), pad_index, dtype=torch.long)\n",
    "    padded_targets = torch.full((len(batch), max_target_len), pad_index, dtype=torch.long)\n",
    "    \n",
    "    for i, (source, target) in enumerate(batch):\n",
    "        padded_sources[i, :len(source)] = source\n",
    "        padded_targets[i, :len(target)] = target\n",
    "    \n",
    "    # Transpose the padded sources and targets to (sequence_length, batch_size)\n",
    "    padded_sources = padded_sources.transpose(0, 1)\n",
    "    padded_targets = padded_targets.transpose(0, 1)\n",
    "    \n",
    "    return padded_sources, padded_targets\n"
   ],
   "id": "ba5ec2f6de76a9de",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\thehe\\AppData\\Local\\Temp\\ipykernel_28644\\487771388.py:9: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  df = pd.read_csv(\"D:\\projects\\Seq2Seq Model\\DataSet\\EN-DE\\EN-DE.txt\", sep = \"\\t\", header= None)[[0,1]].rename(columns = {0:SL, 1:TL})\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T06:35:27.321981Z",
     "start_time": "2024-09-05T06:35:27.317360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Hyper parameters\n",
    "num_epochs = 10\n",
    "learning_Rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "input_size_encoder = len(german_dict)\n",
    "input_Size_Decoder = len(english_dict)\n",
    "output_size = len(english_dict)\n",
    "encoder_embedding_Size = 300\n",
    "decoder_embedding_Size = 300\n",
    "hidden_Size = 1024\n",
    "num_layers = 2 \n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5"
   ],
   "id": "46207f09dc6d4cbc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T06:35:54.909617Z",
     "start_time": "2024-09-05T06:35:27.901087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#intialising the dataset class and dataloader\n",
    "dataset = english_german_translation(english_dict,german_dict)\n",
    "train_dataloader = DataLoader(dataset,batch_size,shuffle=True, collate_fn=custom_collate_fn,drop_last=True)"
   ],
   "id": "c41ccd2080073b93",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T06:35:57.318461Z",
     "start_time": "2024-09-05T06:35:54.918355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#intialising our model\n",
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_Size, hidden_Size, num_layers, encoder_dropout).to(device)\n",
    "decoder_net = Decoder(input_Size_Decoder, decoder_embedding_Size, hidden_Size,output_size, num_layers, decoder_dropout).to(device)\n",
    "model = Seq2Seq(encoder_net,decoder_net).to(device)\n",
    "pad_idx = english_dict['<PAD>']\n",
    "criterion =  nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_Rate)"
   ],
   "id": "ee221a0a10ee8a1b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T09:32:53.969658Z",
     "start_time": "2024-09-05T06:36:39.923176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_history = []\n",
    "PATH = \"./DataSet/Model/model.pth\"\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}\")\n",
    "    i = 0\n",
    "    for batch_idx, (batch_data,batch_labels) in enumerate(train_dataloader):\n",
    "        input_data = batch_data.to(device)\n",
    "        target = batch_labels.to(device)\n",
    "        \n",
    "        output = model(input_data,target)\n",
    "        output = output[1:].reshape(-1,output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output,target)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss after each iteration\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "        loss_history.append(loss.item())\n",
    "        i = i+1\n",
    "        if i ==10000:\n",
    "            break\n",
    "        \n",
    "torch.save(model.state_dict(),PATH)\n",
    "print(\"Model saved successfully.\") \n",
    "        "
   ],
   "id": "175663bce98761e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10\n",
      "Epoch [1/10], Batch [1/88962], Loss: 9.9055\n",
      "Epoch [1/10], Batch [2/88962], Loss: 9.8603\n",
      "Epoch [1/10], Batch [3/88962], Loss: 9.5364\n",
      "Epoch [1/10], Batch [4/88962], Loss: 8.2251\n",
      "Epoch [1/10], Batch [5/88962], Loss: 7.6569\n",
      "Epoch [1/10], Batch [6/88962], Loss: 7.4700\n",
      "Epoch [1/10], Batch [7/88962], Loss: 7.5299\n",
      "Epoch [1/10], Batch [8/88962], Loss: 7.2931\n",
      "Epoch [1/10], Batch [9/88962], Loss: 7.2175\n",
      "Epoch [1/10], Batch [10/88962], Loss: 7.1538\n",
      "Epoch [1/10], Batch [11/88962], Loss: 7.3924\n",
      "Epoch [1/10], Batch [12/88962], Loss: 7.2614\n",
      "Epoch [1/10], Batch [13/88962], Loss: 7.3036\n",
      "Epoch [1/10], Batch [14/88962], Loss: 7.0462\n",
      "Epoch [1/10], Batch [15/88962], Loss: 6.9511\n",
      "Epoch [1/10], Batch [16/88962], Loss: 6.8110\n",
      "Epoch [1/10], Batch [17/88962], Loss: 7.1339\n",
      "Epoch [1/10], Batch [18/88962], Loss: 7.0789\n",
      "Epoch [1/10], Batch [19/88962], Loss: 6.8565\n",
      "Epoch [1/10], Batch [20/88962], Loss: 7.2047\n",
      "Epoch [1/10], Batch [21/88962], Loss: 7.0324\n",
      "Epoch [1/10], Batch [22/88962], Loss: 7.2018\n",
      "Epoch [1/10], Batch [23/88962], Loss: 6.7524\n",
      "Epoch [1/10], Batch [24/88962], Loss: 7.0208\n",
      "Epoch [1/10], Batch [25/88962], Loss: 7.0949\n",
      "Epoch [1/10], Batch [26/88962], Loss: 7.0818\n",
      "Epoch [1/10], Batch [27/88962], Loss: 7.1159\n",
      "Epoch [1/10], Batch [28/88962], Loss: 6.8811\n",
      "Epoch [1/10], Batch [29/88962], Loss: 7.0270\n",
      "Epoch [1/10], Batch [30/88962], Loss: 6.7576\n",
      "Epoch [1/10], Batch [31/88962], Loss: 6.9666\n",
      "Epoch [1/10], Batch [32/88962], Loss: 7.0357\n",
      "Epoch [1/10], Batch [33/88962], Loss: 7.1525\n",
      "Epoch [1/10], Batch [34/88962], Loss: 6.7475\n",
      "Epoch [1/10], Batch [35/88962], Loss: 6.9643\n",
      "Epoch [1/10], Batch [36/88962], Loss: 7.2879\n",
      "Epoch [1/10], Batch [37/88962], Loss: 7.0088\n",
      "Epoch [1/10], Batch [38/88962], Loss: 7.1239\n",
      "Epoch [1/10], Batch [39/88962], Loss: 6.8332\n",
      "Epoch [1/10], Batch [40/88962], Loss: 7.0543\n",
      "Epoch [1/10], Batch [41/88962], Loss: 6.8666\n",
      "Epoch [1/10], Batch [42/88962], Loss: 7.0817\n",
      "Epoch [1/10], Batch [43/88962], Loss: 6.8850\n",
      "Epoch [1/10], Batch [44/88962], Loss: 6.8930\n",
      "Epoch [1/10], Batch [45/88962], Loss: 6.8683\n",
      "Epoch [1/10], Batch [46/88962], Loss: 6.9275\n",
      "Epoch [1/10], Batch [47/88962], Loss: 7.0029\n",
      "Epoch [1/10], Batch [48/88962], Loss: 6.8574\n",
      "Epoch [1/10], Batch [49/88962], Loss: 6.7450\n",
      "Epoch [1/10], Batch [50/88962], Loss: 6.7119\n",
      "Epoch [1/10], Batch [51/88962], Loss: 6.9611\n",
      "Epoch [1/10], Batch [52/88962], Loss: 6.9472\n",
      "Epoch [1/10], Batch [53/88962], Loss: 6.9125\n",
      "Epoch [1/10], Batch [54/88962], Loss: 6.8457\n",
      "Epoch [1/10], Batch [55/88962], Loss: 6.8997\n",
      "Epoch [1/10], Batch [56/88962], Loss: 6.8778\n",
      "Epoch [1/10], Batch [57/88962], Loss: 6.6848\n",
      "Epoch [1/10], Batch [58/88962], Loss: 6.7849\n",
      "Epoch [1/10], Batch [59/88962], Loss: 6.6747\n",
      "Epoch [1/10], Batch [60/88962], Loss: 7.0242\n",
      "Epoch [1/10], Batch [61/88962], Loss: 6.8103\n",
      "Epoch [1/10], Batch [62/88962], Loss: 6.9584\n",
      "Epoch [1/10], Batch [63/88962], Loss: 6.7576\n",
      "Epoch [1/10], Batch [64/88962], Loss: 6.7580\n",
      "Epoch [1/10], Batch [65/88962], Loss: 6.9204\n",
      "Epoch [1/10], Batch [66/88962], Loss: 6.8380\n",
      "Epoch [1/10], Batch [67/88962], Loss: 6.8381\n",
      "Epoch [1/10], Batch [68/88962], Loss: 6.8874\n",
      "Epoch [1/10], Batch [69/88962], Loss: 6.7356\n",
      "Epoch [1/10], Batch [70/88962], Loss: 6.8485\n",
      "Epoch [1/10], Batch [71/88962], Loss: 6.9744\n",
      "Epoch [1/10], Batch [72/88962], Loss: 6.6152\n",
      "Epoch [1/10], Batch [73/88962], Loss: 6.9215\n",
      "Epoch [1/10], Batch [74/88962], Loss: 6.8860\n",
      "Epoch [1/10], Batch [75/88962], Loss: 6.8691\n",
      "Epoch [1/10], Batch [76/88962], Loss: 6.7742\n",
      "Epoch [1/10], Batch [77/88962], Loss: 6.7431\n",
      "Epoch [1/10], Batch [78/88962], Loss: 6.7069\n",
      "Epoch [1/10], Batch [79/88962], Loss: 6.7782\n",
      "Epoch [1/10], Batch [80/88962], Loss: 6.8208\n",
      "Epoch [1/10], Batch [81/88962], Loss: 7.0169\n",
      "Epoch [1/10], Batch [82/88962], Loss: 6.7039\n",
      "Epoch [1/10], Batch [83/88962], Loss: 6.7853\n",
      "Epoch [1/10], Batch [84/88962], Loss: 6.7355\n",
      "Epoch [1/10], Batch [85/88962], Loss: 6.8810\n",
      "Epoch [1/10], Batch [86/88962], Loss: 6.8153\n",
      "Epoch [1/10], Batch [87/88962], Loss: 6.8437\n",
      "Epoch [1/10], Batch [88/88962], Loss: 6.8650\n",
      "Epoch [1/10], Batch [89/88962], Loss: 7.0218\n",
      "Epoch [1/10], Batch [90/88962], Loss: 6.8252\n",
      "Epoch [1/10], Batch [91/88962], Loss: 6.6739\n",
      "Epoch [1/10], Batch [92/88962], Loss: 6.6385\n",
      "Epoch [1/10], Batch [93/88962], Loss: 6.8218\n",
      "Epoch [1/10], Batch [94/88962], Loss: 6.8575\n",
      "Epoch [1/10], Batch [95/88962], Loss: 6.7493\n",
      "Epoch [1/10], Batch [96/88962], Loss: 6.7468\n",
      "Epoch [1/10], Batch [97/88962], Loss: 6.8661\n",
      "Epoch [1/10], Batch [98/88962], Loss: 6.7525\n",
      "Epoch [1/10], Batch [99/88962], Loss: 6.6904\n",
      "Epoch [1/10], Batch [100/88962], Loss: 6.8939\n",
      "Epoch [1/10], Batch [101/88962], Loss: 6.6244\n",
      "Epoch [1/10], Batch [102/88962], Loss: 6.6182\n",
      "Epoch [1/10], Batch [103/88962], Loss: 6.8224\n",
      "Epoch [1/10], Batch [104/88962], Loss: 6.6236\n",
      "Epoch [1/10], Batch [105/88962], Loss: 6.6927\n",
      "Epoch [1/10], Batch [106/88962], Loss: 6.8498\n",
      "Epoch [1/10], Batch [107/88962], Loss: 6.6663\n",
      "Epoch [1/10], Batch [108/88962], Loss: 6.8746\n",
      "Epoch [1/10], Batch [109/88962], Loss: 7.0258\n",
      "Epoch [1/10], Batch [110/88962], Loss: 6.7957\n",
      "Epoch [1/10], Batch [111/88962], Loss: 6.9070\n",
      "Epoch [1/10], Batch [112/88962], Loss: 6.7853\n",
      "Epoch [1/10], Batch [113/88962], Loss: 6.6544\n",
      "Epoch [1/10], Batch [114/88962], Loss: 6.4676\n",
      "Epoch [1/10], Batch [115/88962], Loss: 6.5836\n",
      "Epoch [1/10], Batch [116/88962], Loss: 6.7585\n",
      "Epoch [1/10], Batch [117/88962], Loss: 6.9575\n",
      "Epoch [1/10], Batch [118/88962], Loss: 7.0238\n",
      "Epoch [1/10], Batch [119/88962], Loss: 6.7894\n",
      "Epoch [1/10], Batch [120/88962], Loss: 6.5239\n",
      "Epoch [1/10], Batch [121/88962], Loss: 6.9113\n",
      "Epoch [1/10], Batch [122/88962], Loss: 6.9864\n",
      "Epoch [1/10], Batch [123/88962], Loss: 6.7420\n",
      "Epoch [1/10], Batch [124/88962], Loss: 6.8139\n",
      "Epoch [1/10], Batch [125/88962], Loss: 6.6718\n",
      "Epoch [1/10], Batch [126/88962], Loss: 6.7767\n",
      "Epoch [1/10], Batch [127/88962], Loss: 6.7593\n",
      "Epoch [1/10], Batch [128/88962], Loss: 6.6018\n",
      "Epoch [1/10], Batch [129/88962], Loss: 6.7560\n",
      "Epoch [1/10], Batch [130/88962], Loss: 6.6894\n",
      "Epoch [1/10], Batch [131/88962], Loss: 7.0211\n",
      "Epoch [1/10], Batch [132/88962], Loss: 6.7483\n",
      "Epoch [1/10], Batch [133/88962], Loss: 6.7670\n",
      "Epoch [1/10], Batch [134/88962], Loss: 6.5875\n",
      "Epoch [1/10], Batch [135/88962], Loss: 6.8508\n",
      "Epoch [1/10], Batch [136/88962], Loss: 6.7753\n",
      "Epoch [1/10], Batch [137/88962], Loss: 6.5062\n",
      "Epoch [1/10], Batch [138/88962], Loss: 6.8101\n",
      "Epoch [1/10], Batch [139/88962], Loss: 6.8516\n",
      "Epoch [1/10], Batch [140/88962], Loss: 6.7182\n",
      "Epoch [1/10], Batch [141/88962], Loss: 6.8698\n",
      "Epoch [1/10], Batch [142/88962], Loss: 6.9928\n",
      "Epoch [1/10], Batch [143/88962], Loss: 6.7697\n",
      "Epoch [1/10], Batch [144/88962], Loss: 6.9536\n",
      "Epoch [1/10], Batch [145/88962], Loss: 6.8885\n",
      "Epoch [1/10], Batch [146/88962], Loss: 6.6429\n",
      "Epoch [1/10], Batch [147/88962], Loss: 6.8131\n",
      "Epoch [1/10], Batch [148/88962], Loss: 6.7236\n",
      "Epoch [1/10], Batch [149/88962], Loss: 6.7783\n",
      "Epoch [1/10], Batch [150/88962], Loss: 6.7528\n",
      "Epoch [1/10], Batch [151/88962], Loss: 6.7984\n",
      "Epoch [1/10], Batch [152/88962], Loss: 6.8941\n",
      "Epoch [1/10], Batch [153/88962], Loss: 6.6631\n",
      "Epoch [1/10], Batch [154/88962], Loss: 6.7915\n",
      "Epoch [1/10], Batch [155/88962], Loss: 6.9032\n",
      "Epoch [1/10], Batch [156/88962], Loss: 6.6864\n",
      "Epoch [1/10], Batch [157/88962], Loss: 6.8011\n",
      "Epoch [1/10], Batch [158/88962], Loss: 6.6282\n",
      "Epoch [1/10], Batch [159/88962], Loss: 6.7424\n",
      "Epoch [1/10], Batch [160/88962], Loss: 6.8582\n",
      "Epoch [1/10], Batch [161/88962], Loss: 6.7457\n",
      "Epoch [1/10], Batch [162/88962], Loss: 6.8388\n",
      "Epoch [1/10], Batch [163/88962], Loss: 6.6608\n",
      "Epoch [1/10], Batch [164/88962], Loss: 6.7790\n",
      "Epoch [1/10], Batch [165/88962], Loss: 6.6875\n",
      "Epoch [1/10], Batch [166/88962], Loss: 6.8330\n",
      "Epoch [1/10], Batch [167/88962], Loss: 6.6742\n",
      "Epoch [1/10], Batch [168/88962], Loss: 6.7183\n",
      "Epoch [1/10], Batch [169/88962], Loss: 6.6959\n",
      "Epoch [1/10], Batch [170/88962], Loss: 6.7091\n",
      "Epoch [1/10], Batch [171/88962], Loss: 6.7646\n",
      "Epoch [1/10], Batch [172/88962], Loss: 6.8661\n",
      "Epoch [1/10], Batch [173/88962], Loss: 6.7668\n",
      "Epoch [1/10], Batch [174/88962], Loss: 6.6030\n",
      "Epoch [1/10], Batch [175/88962], Loss: 6.8538\n",
      "Epoch [1/10], Batch [176/88962], Loss: 6.8036\n",
      "Epoch [1/10], Batch [177/88962], Loss: 6.8029\n",
      "Epoch [1/10], Batch [178/88962], Loss: 6.8028\n",
      "Epoch [1/10], Batch [179/88962], Loss: 6.8311\n",
      "Epoch [1/10], Batch [180/88962], Loss: 6.8144\n",
      "Epoch [1/10], Batch [181/88962], Loss: 6.6500\n",
      "Epoch [1/10], Batch [182/88962], Loss: 6.8177\n",
      "Epoch [1/10], Batch [183/88962], Loss: 6.6946\n",
      "Epoch [1/10], Batch [184/88962], Loss: 6.7710\n",
      "Epoch [1/10], Batch [185/88962], Loss: 6.6771\n",
      "Epoch [1/10], Batch [186/88962], Loss: 6.6544\n",
      "Epoch [1/10], Batch [187/88962], Loss: 6.5664\n",
      "Epoch [1/10], Batch [188/88962], Loss: 6.6922\n",
      "Epoch [1/10], Batch [189/88962], Loss: 6.7153\n",
      "Epoch [1/10], Batch [190/88962], Loss: 6.7681\n",
      "Epoch [1/10], Batch [191/88962], Loss: 6.9717\n",
      "Epoch [1/10], Batch [192/88962], Loss: 6.7156\n",
      "Epoch [1/10], Batch [193/88962], Loss: 6.6294\n",
      "Epoch [1/10], Batch [194/88962], Loss: 6.7646\n",
      "Epoch [1/10], Batch [195/88962], Loss: 6.6730\n",
      "Epoch [1/10], Batch [196/88962], Loss: 6.6766\n",
      "Epoch [1/10], Batch [197/88962], Loss: 6.6120\n",
      "Epoch [1/10], Batch [198/88962], Loss: 6.6695\n",
      "Epoch [1/10], Batch [199/88962], Loss: 6.7152\n",
      "Epoch [1/10], Batch [200/88962], Loss: 6.5847\n",
      "Epoch [1/10], Batch [201/88962], Loss: 6.5659\n",
      "Epoch [1/10], Batch [202/88962], Loss: 6.7705\n",
      "Epoch [1/10], Batch [203/88962], Loss: 6.4863\n",
      "Epoch [1/10], Batch [204/88962], Loss: 6.6099\n",
      "Epoch [1/10], Batch [205/88962], Loss: 6.7345\n",
      "Epoch [1/10], Batch [206/88962], Loss: 6.6313\n",
      "Epoch [1/10], Batch [207/88962], Loss: 6.7879\n",
      "Epoch [1/10], Batch [208/88962], Loss: 6.6041\n",
      "Epoch [1/10], Batch [209/88962], Loss: 6.7520\n",
      "Epoch [1/10], Batch [210/88962], Loss: 6.7400\n",
      "Epoch [1/10], Batch [211/88962], Loss: 6.8416\n",
      "Epoch [1/10], Batch [212/88962], Loss: 6.5759\n",
      "Epoch [1/10], Batch [213/88962], Loss: 6.6392\n",
      "Epoch [1/10], Batch [214/88962], Loss: 6.5591\n",
      "Epoch [1/10], Batch [215/88962], Loss: 6.7492\n",
      "Epoch [1/10], Batch [216/88962], Loss: 6.5505\n",
      "Epoch [1/10], Batch [217/88962], Loss: 6.5858\n",
      "Epoch [1/10], Batch [218/88962], Loss: 6.4746\n",
      "Epoch [1/10], Batch [219/88962], Loss: 6.7145\n",
      "Epoch [1/10], Batch [220/88962], Loss: 6.5808\n",
      "Epoch [1/10], Batch [221/88962], Loss: 6.6275\n",
      "Epoch [1/10], Batch [222/88962], Loss: 6.6033\n",
      "Epoch [1/10], Batch [223/88962], Loss: 6.5351\n",
      "Epoch [1/10], Batch [224/88962], Loss: 6.7402\n",
      "Epoch [1/10], Batch [225/88962], Loss: 6.8432\n",
      "Epoch [1/10], Batch [226/88962], Loss: 6.6195\n",
      "Epoch [1/10], Batch [227/88962], Loss: 6.5465\n",
      "Epoch [1/10], Batch [228/88962], Loss: 6.7753\n",
      "Epoch [1/10], Batch [229/88962], Loss: 6.6396\n",
      "Epoch [1/10], Batch [230/88962], Loss: 6.7669\n",
      "Epoch [1/10], Batch [231/88962], Loss: 6.6493\n",
      "Epoch [1/10], Batch [232/88962], Loss: 6.7607\n",
      "Epoch [1/10], Batch [233/88962], Loss: 6.3993\n",
      "Epoch [1/10], Batch [234/88962], Loss: 6.5335\n",
      "Epoch [1/10], Batch [235/88962], Loss: 6.5386\n",
      "Epoch [1/10], Batch [236/88962], Loss: 6.8064\n",
      "Epoch [1/10], Batch [237/88962], Loss: 6.6527\n",
      "Epoch [1/10], Batch [238/88962], Loss: 6.5293\n",
      "Epoch [1/10], Batch [239/88962], Loss: 6.7614\n",
      "Epoch [1/10], Batch [240/88962], Loss: 6.5971\n",
      "Epoch [1/10], Batch [241/88962], Loss: 6.7016\n",
      "Epoch [1/10], Batch [242/88962], Loss: 6.7165\n",
      "Epoch [1/10], Batch [243/88962], Loss: 6.8491\n",
      "Epoch [1/10], Batch [244/88962], Loss: 6.5948\n",
      "Epoch [1/10], Batch [245/88962], Loss: 6.5383\n",
      "Epoch [1/10], Batch [246/88962], Loss: 6.5337\n",
      "Epoch [1/10], Batch [247/88962], Loss: 6.5680\n",
      "Epoch [1/10], Batch [248/88962], Loss: 6.6969\n",
      "Epoch [1/10], Batch [249/88962], Loss: 6.5795\n",
      "Epoch [1/10], Batch [250/88962], Loss: 6.9131\n",
      "Epoch [1/10], Batch [251/88962], Loss: 6.4861\n",
      "Epoch [1/10], Batch [252/88962], Loss: 6.6810\n",
      "Epoch [1/10], Batch [253/88962], Loss: 6.6160\n",
      "Epoch [1/10], Batch [254/88962], Loss: 6.5019\n",
      "Epoch [1/10], Batch [255/88962], Loss: 6.2829\n",
      "Epoch [1/10], Batch [256/88962], Loss: 6.6322\n",
      "Epoch [1/10], Batch [257/88962], Loss: 6.5738\n",
      "Epoch [1/10], Batch [258/88962], Loss: 6.7609\n",
      "Epoch [1/10], Batch [259/88962], Loss: 6.8176\n",
      "Epoch [1/10], Batch [260/88962], Loss: 6.6753\n",
      "Epoch [1/10], Batch [261/88962], Loss: 6.5481\n",
      "Epoch [1/10], Batch [262/88962], Loss: 6.5400\n",
      "Epoch [1/10], Batch [263/88962], Loss: 6.3991\n",
      "Epoch [1/10], Batch [264/88962], Loss: 6.6584\n",
      "Epoch [1/10], Batch [265/88962], Loss: 6.7908\n",
      "Epoch [1/10], Batch [266/88962], Loss: 6.6737\n",
      "Epoch [1/10], Batch [267/88962], Loss: 6.5105\n",
      "Epoch [1/10], Batch [268/88962], Loss: 6.6530\n",
      "Epoch [1/10], Batch [269/88962], Loss: 6.5367\n",
      "Epoch [1/10], Batch [270/88962], Loss: 6.6540\n",
      "Epoch [1/10], Batch [271/88962], Loss: 6.6121\n",
      "Epoch [1/10], Batch [272/88962], Loss: 6.5603\n",
      "Epoch [1/10], Batch [273/88962], Loss: 6.7740\n",
      "Epoch [1/10], Batch [274/88962], Loss: 6.6800\n",
      "Epoch [1/10], Batch [275/88962], Loss: 6.5355\n",
      "Epoch [1/10], Batch [276/88962], Loss: 6.3807\n",
      "Epoch [1/10], Batch [277/88962], Loss: 6.3644\n",
      "Epoch [1/10], Batch [278/88962], Loss: 6.5616\n",
      "Epoch [1/10], Batch [279/88962], Loss: 6.5137\n",
      "Epoch [1/10], Batch [280/88962], Loss: 6.6266\n",
      "Epoch [1/10], Batch [281/88962], Loss: 6.6313\n",
      "Epoch [1/10], Batch [282/88962], Loss: 6.5194\n",
      "Epoch [1/10], Batch [283/88962], Loss: 6.5547\n",
      "Epoch [1/10], Batch [284/88962], Loss: 6.8351\n",
      "Epoch [1/10], Batch [285/88962], Loss: 6.4701\n",
      "Epoch [1/10], Batch [286/88962], Loss: 6.5916\n",
      "Epoch [1/10], Batch [287/88962], Loss: 6.5192\n",
      "Epoch [1/10], Batch [288/88962], Loss: 6.7221\n",
      "Epoch [1/10], Batch [289/88962], Loss: 6.6966\n",
      "Epoch [1/10], Batch [290/88962], Loss: 6.5556\n",
      "Epoch [1/10], Batch [291/88962], Loss: 6.6099\n",
      "Epoch [1/10], Batch [292/88962], Loss: 6.4095\n",
      "Epoch [1/10], Batch [293/88962], Loss: 6.5489\n",
      "Epoch [1/10], Batch [294/88962], Loss: 6.7224\n",
      "Epoch [1/10], Batch [295/88962], Loss: 6.5522\n",
      "Epoch [1/10], Batch [296/88962], Loss: 6.4797\n",
      "Epoch [1/10], Batch [297/88962], Loss: 6.7449\n",
      "Epoch [1/10], Batch [298/88962], Loss: 6.6904\n",
      "Epoch [1/10], Batch [299/88962], Loss: 6.6462\n",
      "Epoch [1/10], Batch [300/88962], Loss: 6.5838\n",
      "Epoch [1/10], Batch [301/88962], Loss: 6.3804\n",
      "Epoch [1/10], Batch [302/88962], Loss: 6.2646\n",
      "Epoch [1/10], Batch [303/88962], Loss: 6.5148\n",
      "Epoch [1/10], Batch [304/88962], Loss: 6.6563\n",
      "Epoch [1/10], Batch [305/88962], Loss: 6.4145\n",
      "Epoch [1/10], Batch [306/88962], Loss: 6.4036\n",
      "Epoch [1/10], Batch [307/88962], Loss: 6.3704\n",
      "Epoch [1/10], Batch [308/88962], Loss: 6.6708\n",
      "Epoch [1/10], Batch [309/88962], Loss: 6.4273\n",
      "Epoch [1/10], Batch [310/88962], Loss: 6.5520\n",
      "Epoch [1/10], Batch [311/88962], Loss: 6.6364\n",
      "Epoch [1/10], Batch [312/88962], Loss: 6.3227\n",
      "Epoch [1/10], Batch [313/88962], Loss: 6.6169\n",
      "Epoch [1/10], Batch [314/88962], Loss: 6.5942\n",
      "Epoch [1/10], Batch [315/88962], Loss: 6.3321\n",
      "Epoch [1/10], Batch [316/88962], Loss: 6.4897\n",
      "Epoch [1/10], Batch [317/88962], Loss: 6.4180\n",
      "Epoch [1/10], Batch [318/88962], Loss: 6.3977\n",
      "Epoch [1/10], Batch [319/88962], Loss: 6.5352\n",
      "Epoch [1/10], Batch [320/88962], Loss: 6.5766\n",
      "Epoch [1/10], Batch [321/88962], Loss: 6.5106\n",
      "Epoch [1/10], Batch [322/88962], Loss: 6.5483\n",
      "Epoch [1/10], Batch [323/88962], Loss: 6.3634\n",
      "Epoch [1/10], Batch [324/88962], Loss: 6.5715\n",
      "Epoch [1/10], Batch [325/88962], Loss: 6.6214\n",
      "Epoch [1/10], Batch [326/88962], Loss: 6.7678\n",
      "Epoch [1/10], Batch [327/88962], Loss: 6.3257\n",
      "Epoch [1/10], Batch [328/88962], Loss: 6.5454\n",
      "Epoch [1/10], Batch [329/88962], Loss: 6.7663\n",
      "Epoch [1/10], Batch [330/88962], Loss: 6.7724\n",
      "Epoch [1/10], Batch [331/88962], Loss: 6.3236\n",
      "Epoch [1/10], Batch [332/88962], Loss: 6.8548\n",
      "Epoch [1/10], Batch [333/88962], Loss: 6.2530\n",
      "Epoch [1/10], Batch [334/88962], Loss: 6.6948\n",
      "Epoch [1/10], Batch [335/88962], Loss: 6.3961\n",
      "Epoch [1/10], Batch [336/88962], Loss: 6.6995\n",
      "Epoch [1/10], Batch [337/88962], Loss: 6.5581\n",
      "Epoch [1/10], Batch [338/88962], Loss: 6.4379\n",
      "Epoch [1/10], Batch [339/88962], Loss: 6.2703\n",
      "Epoch [1/10], Batch [340/88962], Loss: 6.4637\n",
      "Epoch [1/10], Batch [341/88962], Loss: 6.4989\n",
      "Epoch [1/10], Batch [342/88962], Loss: 6.4525\n",
      "Epoch [1/10], Batch [343/88962], Loss: 6.6527\n",
      "Epoch [1/10], Batch [344/88962], Loss: 6.8328\n",
      "Epoch [1/10], Batch [345/88962], Loss: 6.5957\n",
      "Epoch [1/10], Batch [346/88962], Loss: 6.5102\n",
      "Epoch [1/10], Batch [347/88962], Loss: 6.3852\n",
      "Epoch [1/10], Batch [348/88962], Loss: 6.3515\n",
      "Epoch [1/10], Batch [349/88962], Loss: 6.2933\n",
      "Epoch [1/10], Batch [350/88962], Loss: 6.6141\n",
      "Epoch [1/10], Batch [351/88962], Loss: 6.5342\n",
      "Epoch [1/10], Batch [352/88962], Loss: 6.4951\n",
      "Epoch [1/10], Batch [353/88962], Loss: 6.4158\n",
      "Epoch [1/10], Batch [354/88962], Loss: 6.7589\n",
      "Epoch [1/10], Batch [355/88962], Loss: 6.2884\n",
      "Epoch [1/10], Batch [356/88962], Loss: 6.4143\n",
      "Epoch [1/10], Batch [357/88962], Loss: 6.6544\n",
      "Epoch [1/10], Batch [358/88962], Loss: 6.4654\n",
      "Epoch [1/10], Batch [359/88962], Loss: 6.4054\n",
      "Epoch [1/10], Batch [360/88962], Loss: 6.6502\n",
      "Epoch [1/10], Batch [361/88962], Loss: 6.3198\n",
      "Epoch [1/10], Batch [362/88962], Loss: 6.4904\n",
      "Epoch [1/10], Batch [363/88962], Loss: 6.5171\n",
      "Epoch [1/10], Batch [364/88962], Loss: 6.3529\n",
      "Epoch [1/10], Batch [365/88962], Loss: 6.4183\n",
      "Epoch [1/10], Batch [366/88962], Loss: 6.4614\n",
      "Epoch [1/10], Batch [367/88962], Loss: 6.4640\n",
      "Epoch [1/10], Batch [368/88962], Loss: 6.5248\n",
      "Epoch [1/10], Batch [369/88962], Loss: 6.5635\n",
      "Epoch [1/10], Batch [370/88962], Loss: 6.6170\n",
      "Epoch [1/10], Batch [371/88962], Loss: 6.6319\n",
      "Epoch [1/10], Batch [372/88962], Loss: 6.3028\n",
      "Epoch [1/10], Batch [373/88962], Loss: 6.4147\n",
      "Epoch [1/10], Batch [374/88962], Loss: 6.5019\n",
      "Epoch [1/10], Batch [375/88962], Loss: 6.5074\n",
      "Epoch [1/10], Batch [376/88962], Loss: 6.6017\n",
      "Epoch [1/10], Batch [377/88962], Loss: 6.3642\n",
      "Epoch [1/10], Batch [378/88962], Loss: 6.4177\n",
      "Epoch [1/10], Batch [379/88962], Loss: 6.3228\n",
      "Epoch [1/10], Batch [380/88962], Loss: 6.4687\n",
      "Epoch [1/10], Batch [381/88962], Loss: 6.3831\n",
      "Epoch [1/10], Batch [382/88962], Loss: 6.7696\n",
      "Epoch [1/10], Batch [383/88962], Loss: 6.5059\n",
      "Epoch [1/10], Batch [384/88962], Loss: 6.3401\n",
      "Epoch [1/10], Batch [385/88962], Loss: 6.5390\n",
      "Epoch [1/10], Batch [386/88962], Loss: 6.6079\n",
      "Epoch [1/10], Batch [387/88962], Loss: 6.2083\n",
      "Epoch [1/10], Batch [388/88962], Loss: 6.5647\n",
      "Epoch [1/10], Batch [389/88962], Loss: 6.6537\n",
      "Epoch [1/10], Batch [390/88962], Loss: 6.1486\n",
      "Epoch [1/10], Batch [391/88962], Loss: 6.3190\n",
      "Epoch [1/10], Batch [392/88962], Loss: 6.3373\n",
      "Epoch [1/10], Batch [393/88962], Loss: 6.4916\n",
      "Epoch [1/10], Batch [394/88962], Loss: 6.3429\n",
      "Epoch [1/10], Batch [395/88962], Loss: 6.6493\n",
      "Epoch [1/10], Batch [396/88962], Loss: 6.3783\n",
      "Epoch [1/10], Batch [397/88962], Loss: 6.3573\n",
      "Epoch [1/10], Batch [398/88962], Loss: 6.3532\n",
      "Epoch [1/10], Batch [399/88962], Loss: 6.3920\n",
      "Epoch [1/10], Batch [400/88962], Loss: 6.1051\n",
      "Epoch [1/10], Batch [401/88962], Loss: 6.6423\n",
      "Epoch [1/10], Batch [402/88962], Loss: 6.6916\n",
      "Epoch [1/10], Batch [403/88962], Loss: 6.5075\n",
      "Epoch [1/10], Batch [404/88962], Loss: 6.4513\n",
      "Epoch [1/10], Batch [405/88962], Loss: 6.6253\n",
      "Epoch [1/10], Batch [406/88962], Loss: 6.4909\n",
      "Epoch [1/10], Batch [407/88962], Loss: 6.4922\n",
      "Epoch [1/10], Batch [408/88962], Loss: 6.5253\n",
      "Epoch [1/10], Batch [409/88962], Loss: 6.3605\n",
      "Epoch [1/10], Batch [410/88962], Loss: 6.4218\n",
      "Epoch [1/10], Batch [411/88962], Loss: 6.4951\n",
      "Epoch [1/10], Batch [412/88962], Loss: 6.7741\n",
      "Epoch [1/10], Batch [413/88962], Loss: 6.8468\n",
      "Epoch [1/10], Batch [414/88962], Loss: 6.4450\n",
      "Epoch [1/10], Batch [415/88962], Loss: 6.3842\n",
      "Epoch [1/10], Batch [416/88962], Loss: 6.3300\n",
      "Epoch [1/10], Batch [417/88962], Loss: 6.4926\n",
      "Epoch [1/10], Batch [418/88962], Loss: 6.3107\n",
      "Epoch [1/10], Batch [419/88962], Loss: 6.3988\n",
      "Epoch [1/10], Batch [420/88962], Loss: 6.3982\n",
      "Epoch [1/10], Batch [421/88962], Loss: 6.4381\n",
      "Epoch [1/10], Batch [422/88962], Loss: 6.5346\n",
      "Epoch [1/10], Batch [423/88962], Loss: 6.2786\n",
      "Epoch [1/10], Batch [424/88962], Loss: 6.1297\n",
      "Epoch [1/10], Batch [425/88962], Loss: 6.1110\n",
      "Epoch [1/10], Batch [426/88962], Loss: 6.2286\n",
      "Epoch [1/10], Batch [427/88962], Loss: 6.3655\n",
      "Epoch [1/10], Batch [428/88962], Loss: 6.3297\n",
      "Epoch [1/10], Batch [429/88962], Loss: 6.4487\n",
      "Epoch [1/10], Batch [430/88962], Loss: 6.7652\n",
      "Epoch [1/10], Batch [431/88962], Loss: 6.4879\n",
      "Epoch [1/10], Batch [432/88962], Loss: 6.5014\n",
      "Epoch [1/10], Batch [433/88962], Loss: 6.3688\n",
      "Epoch [1/10], Batch [434/88962], Loss: 6.0768\n",
      "Epoch [1/10], Batch [435/88962], Loss: 6.3369\n",
      "Epoch [1/10], Batch [436/88962], Loss: 6.4246\n",
      "Epoch [1/10], Batch [437/88962], Loss: 6.1711\n",
      "Epoch [1/10], Batch [438/88962], Loss: 6.3971\n",
      "Epoch [1/10], Batch [439/88962], Loss: 6.3471\n",
      "Epoch [1/10], Batch [440/88962], Loss: 6.3865\n",
      "Epoch [1/10], Batch [441/88962], Loss: 5.9188\n",
      "Epoch [1/10], Batch [442/88962], Loss: 6.3198\n",
      "Epoch [1/10], Batch [443/88962], Loss: 6.4198\n",
      "Epoch [1/10], Batch [444/88962], Loss: 6.5036\n",
      "Epoch [1/10], Batch [445/88962], Loss: 6.2560\n",
      "Epoch [1/10], Batch [446/88962], Loss: 6.4624\n",
      "Epoch [1/10], Batch [447/88962], Loss: 6.3462\n",
      "Epoch [1/10], Batch [448/88962], Loss: 6.6234\n",
      "Epoch [1/10], Batch [449/88962], Loss: 6.2743\n",
      "Epoch [1/10], Batch [450/88962], Loss: 6.0333\n",
      "Epoch [1/10], Batch [451/88962], Loss: 6.1890\n",
      "Epoch [1/10], Batch [452/88962], Loss: 6.3828\n",
      "Epoch [1/10], Batch [453/88962], Loss: 6.3443\n",
      "Epoch [1/10], Batch [454/88962], Loss: 6.2238\n",
      "Epoch [1/10], Batch [455/88962], Loss: 6.5799\n",
      "Epoch [1/10], Batch [456/88962], Loss: 6.3949\n",
      "Epoch [1/10], Batch [457/88962], Loss: 6.4621\n",
      "Epoch [1/10], Batch [458/88962], Loss: 6.3947\n",
      "Epoch [1/10], Batch [459/88962], Loss: 6.2278\n",
      "Epoch [1/10], Batch [460/88962], Loss: 6.6589\n",
      "Epoch [1/10], Batch [461/88962], Loss: 6.4896\n",
      "Epoch [1/10], Batch [462/88962], Loss: 6.0791\n",
      "Epoch [1/10], Batch [463/88962], Loss: 6.4630\n",
      "Epoch [1/10], Batch [464/88962], Loss: 6.2837\n",
      "Epoch [1/10], Batch [465/88962], Loss: 6.1047\n",
      "Epoch [1/10], Batch [466/88962], Loss: 6.4322\n",
      "Epoch [1/10], Batch [467/88962], Loss: 6.5947\n",
      "Epoch [1/10], Batch [468/88962], Loss: 6.4240\n",
      "Epoch [1/10], Batch [469/88962], Loss: 6.4704\n",
      "Epoch [1/10], Batch [470/88962], Loss: 6.3316\n",
      "Epoch [1/10], Batch [471/88962], Loss: 6.5338\n",
      "Epoch [1/10], Batch [472/88962], Loss: 6.6499\n",
      "Epoch [1/10], Batch [473/88962], Loss: 6.2527\n",
      "Epoch [1/10], Batch [474/88962], Loss: 6.2920\n",
      "Epoch [1/10], Batch [475/88962], Loss: 6.5459\n",
      "Epoch [1/10], Batch [476/88962], Loss: 6.2027\n",
      "Epoch [1/10], Batch [477/88962], Loss: 6.1134\n",
      "Epoch [1/10], Batch [478/88962], Loss: 6.0209\n",
      "Epoch [1/10], Batch [479/88962], Loss: 6.3453\n",
      "Epoch [1/10], Batch [480/88962], Loss: 6.2496\n",
      "Epoch [1/10], Batch [481/88962], Loss: 6.4232\n",
      "Epoch [1/10], Batch [482/88962], Loss: 6.0981\n",
      "Epoch [1/10], Batch [483/88962], Loss: 6.2369\n",
      "Epoch [1/10], Batch [484/88962], Loss: 6.0847\n",
      "Epoch [1/10], Batch [485/88962], Loss: 6.2520\n",
      "Epoch [1/10], Batch [486/88962], Loss: 6.3213\n",
      "Epoch [1/10], Batch [487/88962], Loss: 6.1228\n",
      "Epoch [1/10], Batch [488/88962], Loss: 6.3465\n",
      "Epoch [1/10], Batch [489/88962], Loss: 6.0825\n",
      "Epoch [1/10], Batch [490/88962], Loss: 6.0956\n",
      "Epoch [1/10], Batch [491/88962], Loss: 5.9920\n",
      "Epoch [1/10], Batch [492/88962], Loss: 6.4092\n",
      "Epoch [1/10], Batch [493/88962], Loss: 6.1937\n",
      "Epoch [1/10], Batch [494/88962], Loss: 6.2687\n",
      "Epoch [1/10], Batch [495/88962], Loss: 6.0991\n",
      "Epoch [1/10], Batch [496/88962], Loss: 6.3129\n",
      "Epoch [1/10], Batch [497/88962], Loss: 6.3413\n",
      "Epoch [1/10], Batch [498/88962], Loss: 6.2940\n",
      "Epoch [1/10], Batch [499/88962], Loss: 6.3712\n",
      "Epoch [1/10], Batch [500/88962], Loss: 6.4412\n",
      "Epoch [1/10], Batch [501/88962], Loss: 6.4435\n",
      "Epoch [1/10], Batch [502/88962], Loss: 6.2314\n",
      "Epoch [1/10], Batch [503/88962], Loss: 6.4007\n",
      "Epoch [1/10], Batch [504/88962], Loss: 6.3077\n",
      "Epoch [1/10], Batch [505/88962], Loss: 6.3730\n",
      "Epoch [1/10], Batch [506/88962], Loss: 5.8440\n",
      "Epoch [1/10], Batch [507/88962], Loss: 6.0726\n",
      "Epoch [1/10], Batch [508/88962], Loss: 6.4835\n",
      "Epoch [1/10], Batch [509/88962], Loss: 6.1740\n",
      "Epoch [1/10], Batch [510/88962], Loss: 6.1361\n",
      "Epoch [1/10], Batch [511/88962], Loss: 5.8724\n",
      "Epoch [1/10], Batch [512/88962], Loss: 5.9607\n",
      "Epoch [1/10], Batch [513/88962], Loss: 5.9844\n",
      "Epoch [1/10], Batch [514/88962], Loss: 6.2911\n",
      "Epoch [1/10], Batch [515/88962], Loss: 6.1059\n",
      "Epoch [1/10], Batch [516/88962], Loss: 6.4366\n",
      "Epoch [1/10], Batch [517/88962], Loss: 5.8587\n",
      "Epoch [1/10], Batch [518/88962], Loss: 6.1372\n",
      "Epoch [1/10], Batch [519/88962], Loss: 6.2552\n",
      "Epoch [1/10], Batch [520/88962], Loss: 6.1303\n",
      "Epoch [1/10], Batch [521/88962], Loss: 6.0900\n",
      "Epoch [1/10], Batch [522/88962], Loss: 6.2643\n",
      "Epoch [1/10], Batch [523/88962], Loss: 6.0781\n",
      "Epoch [1/10], Batch [524/88962], Loss: 6.0463\n",
      "Epoch [1/10], Batch [525/88962], Loss: 6.0311\n",
      "Epoch [1/10], Batch [526/88962], Loss: 6.2567\n",
      "Epoch [1/10], Batch [527/88962], Loss: 6.4091\n",
      "Epoch [1/10], Batch [528/88962], Loss: 6.3146\n",
      "Epoch [1/10], Batch [529/88962], Loss: 6.0755\n",
      "Epoch [1/10], Batch [530/88962], Loss: 6.2069\n",
      "Epoch [1/10], Batch [531/88962], Loss: 6.2580\n",
      "Epoch [1/10], Batch [532/88962], Loss: 6.0917\n",
      "Epoch [1/10], Batch [533/88962], Loss: 6.2739\n",
      "Epoch [1/10], Batch [534/88962], Loss: 6.3544\n",
      "Epoch [1/10], Batch [535/88962], Loss: 6.3583\n",
      "Epoch [1/10], Batch [536/88962], Loss: 6.3818\n",
      "Epoch [1/10], Batch [537/88962], Loss: 6.1763\n",
      "Epoch [1/10], Batch [538/88962], Loss: 6.3540\n",
      "Epoch [1/10], Batch [539/88962], Loss: 6.0904\n",
      "Epoch [1/10], Batch [540/88962], Loss: 6.5078\n",
      "Epoch [1/10], Batch [541/88962], Loss: 6.1584\n",
      "Epoch [1/10], Batch [542/88962], Loss: 6.2726\n",
      "Epoch [1/10], Batch [543/88962], Loss: 5.9124\n",
      "Epoch [1/10], Batch [544/88962], Loss: 6.0272\n",
      "Epoch [1/10], Batch [545/88962], Loss: 6.4353\n",
      "Epoch [1/10], Batch [546/88962], Loss: 6.5183\n",
      "Epoch [1/10], Batch [547/88962], Loss: 6.0904\n",
      "Epoch [1/10], Batch [548/88962], Loss: 6.1943\n",
      "Epoch [1/10], Batch [549/88962], Loss: 6.3427\n",
      "Epoch [1/10], Batch [550/88962], Loss: 5.9588\n",
      "Epoch [1/10], Batch [551/88962], Loss: 6.0889\n",
      "Epoch [1/10], Batch [552/88962], Loss: 6.1611\n",
      "Epoch [1/10], Batch [553/88962], Loss: 6.3423\n",
      "Epoch [1/10], Batch [554/88962], Loss: 6.2359\n",
      "Epoch [1/10], Batch [555/88962], Loss: 6.2725\n",
      "Epoch [1/10], Batch [556/88962], Loss: 6.2030\n",
      "Epoch [1/10], Batch [557/88962], Loss: 6.0993\n",
      "Epoch [1/10], Batch [558/88962], Loss: 6.2891\n",
      "Epoch [1/10], Batch [559/88962], Loss: 6.7137\n",
      "Epoch [1/10], Batch [560/88962], Loss: 6.0904\n",
      "Epoch [1/10], Batch [561/88962], Loss: 5.9601\n",
      "Epoch [1/10], Batch [562/88962], Loss: 5.8835\n",
      "Epoch [1/10], Batch [563/88962], Loss: 5.7192\n",
      "Epoch [1/10], Batch [564/88962], Loss: 6.1082\n",
      "Epoch [1/10], Batch [565/88962], Loss: 6.2557\n",
      "Epoch [1/10], Batch [566/88962], Loss: 6.3209\n",
      "Epoch [1/10], Batch [567/88962], Loss: 6.1269\n",
      "Epoch [1/10], Batch [568/88962], Loss: 6.2464\n",
      "Epoch [1/10], Batch [569/88962], Loss: 6.3581\n",
      "Epoch [1/10], Batch [570/88962], Loss: 5.9767\n",
      "Epoch [1/10], Batch [571/88962], Loss: 6.2372\n",
      "Epoch [1/10], Batch [572/88962], Loss: 6.4572\n",
      "Epoch [1/10], Batch [573/88962], Loss: 6.0730\n",
      "Epoch [1/10], Batch [574/88962], Loss: 5.9136\n",
      "Epoch [1/10], Batch [575/88962], Loss: 6.2821\n",
      "Epoch [1/10], Batch [576/88962], Loss: 6.1478\n",
      "Epoch [1/10], Batch [577/88962], Loss: 6.2095\n",
      "Epoch [1/10], Batch [578/88962], Loss: 6.3275\n",
      "Epoch [1/10], Batch [579/88962], Loss: 5.9126\n",
      "Epoch [1/10], Batch [580/88962], Loss: 5.9390\n",
      "Epoch [1/10], Batch [581/88962], Loss: 6.2126\n",
      "Epoch [1/10], Batch [582/88962], Loss: 6.6958\n",
      "Epoch [1/10], Batch [583/88962], Loss: 5.9995\n",
      "Epoch [1/10], Batch [584/88962], Loss: 6.1798\n",
      "Epoch [1/10], Batch [585/88962], Loss: 6.2027\n",
      "Epoch [1/10], Batch [586/88962], Loss: 6.0763\n",
      "Epoch [1/10], Batch [587/88962], Loss: 5.9043\n",
      "Epoch [1/10], Batch [588/88962], Loss: 6.0761\n",
      "Epoch [1/10], Batch [589/88962], Loss: 6.0289\n",
      "Epoch [1/10], Batch [590/88962], Loss: 6.4616\n",
      "Epoch [1/10], Batch [591/88962], Loss: 6.0328\n",
      "Epoch [1/10], Batch [592/88962], Loss: 6.2870\n",
      "Epoch [1/10], Batch [593/88962], Loss: 6.0273\n",
      "Epoch [1/10], Batch [594/88962], Loss: 6.0997\n",
      "Epoch [1/10], Batch [595/88962], Loss: 6.2345\n",
      "Epoch [1/10], Batch [596/88962], Loss: 6.2825\n",
      "Epoch [1/10], Batch [597/88962], Loss: 5.8448\n",
      "Epoch [1/10], Batch [598/88962], Loss: 5.9165\n",
      "Epoch [1/10], Batch [599/88962], Loss: 6.0956\n",
      "Epoch [1/10], Batch [600/88962], Loss: 6.1215\n",
      "Epoch [1/10], Batch [601/88962], Loss: 6.0883\n",
      "Epoch [1/10], Batch [602/88962], Loss: 5.9939\n",
      "Epoch [1/10], Batch [603/88962], Loss: 6.2664\n",
      "Epoch [1/10], Batch [604/88962], Loss: 6.0682\n",
      "Epoch [1/10], Batch [605/88962], Loss: 6.4462\n",
      "Epoch [1/10], Batch [606/88962], Loss: 6.3255\n",
      "Epoch [1/10], Batch [607/88962], Loss: 6.2249\n",
      "Epoch [1/10], Batch [608/88962], Loss: 6.5021\n",
      "Epoch [1/10], Batch [609/88962], Loss: 5.8223\n",
      "Epoch [1/10], Batch [610/88962], Loss: 5.9308\n",
      "Epoch [1/10], Batch [611/88962], Loss: 6.0414\n",
      "Epoch [1/10], Batch [612/88962], Loss: 6.2889\n",
      "Epoch [1/10], Batch [613/88962], Loss: 6.1207\n",
      "Epoch [1/10], Batch [614/88962], Loss: 6.4207\n",
      "Epoch [1/10], Batch [615/88962], Loss: 6.2026\n",
      "Epoch [1/10], Batch [616/88962], Loss: 6.4075\n",
      "Epoch [1/10], Batch [617/88962], Loss: 6.1416\n",
      "Epoch [1/10], Batch [618/88962], Loss: 6.0509\n",
      "Epoch [1/10], Batch [619/88962], Loss: 6.2544\n",
      "Epoch [1/10], Batch [620/88962], Loss: 6.5015\n",
      "Epoch [1/10], Batch [621/88962], Loss: 6.1738\n",
      "Epoch [1/10], Batch [622/88962], Loss: 6.3759\n",
      "Epoch [1/10], Batch [623/88962], Loss: 6.3708\n",
      "Epoch [1/10], Batch [624/88962], Loss: 6.1556\n",
      "Epoch [1/10], Batch [625/88962], Loss: 6.1330\n",
      "Epoch [1/10], Batch [626/88962], Loss: 6.1449\n",
      "Epoch [1/10], Batch [627/88962], Loss: 5.9800\n",
      "Epoch [1/10], Batch [628/88962], Loss: 6.1344\n",
      "Epoch [1/10], Batch [629/88962], Loss: 5.8633\n",
      "Epoch [1/10], Batch [630/88962], Loss: 5.9027\n",
      "Epoch [1/10], Batch [631/88962], Loss: 6.1372\n",
      "Epoch [1/10], Batch [632/88962], Loss: 6.1020\n",
      "Epoch [1/10], Batch [633/88962], Loss: 6.0616\n",
      "Epoch [1/10], Batch [634/88962], Loss: 6.3484\n",
      "Epoch [1/10], Batch [635/88962], Loss: 6.3527\n",
      "Epoch [1/10], Batch [636/88962], Loss: 6.0199\n",
      "Epoch [1/10], Batch [637/88962], Loss: 6.1591\n",
      "Epoch [1/10], Batch [638/88962], Loss: 5.9248\n",
      "Epoch [1/10], Batch [639/88962], Loss: 5.9385\n",
      "Epoch [1/10], Batch [640/88962], Loss: 5.9749\n",
      "Epoch [1/10], Batch [641/88962], Loss: 6.2336\n",
      "Epoch [1/10], Batch [642/88962], Loss: 6.1607\n",
      "Epoch [1/10], Batch [643/88962], Loss: 6.0654\n",
      "Epoch [1/10], Batch [644/88962], Loss: 5.8417\n",
      "Epoch [1/10], Batch [645/88962], Loss: 6.1401\n",
      "Epoch [1/10], Batch [646/88962], Loss: 6.5129\n",
      "Epoch [1/10], Batch [647/88962], Loss: 5.9686\n",
      "Epoch [1/10], Batch [648/88962], Loss: 6.2803\n",
      "Epoch [1/10], Batch [649/88962], Loss: 5.9323\n",
      "Epoch [1/10], Batch [650/88962], Loss: 6.0441\n",
      "Epoch [1/10], Batch [651/88962], Loss: 6.1007\n",
      "Epoch [1/10], Batch [652/88962], Loss: 5.9824\n",
      "Epoch [1/10], Batch [653/88962], Loss: 6.2251\n",
      "Epoch [1/10], Batch [654/88962], Loss: 5.9319\n",
      "Epoch [1/10], Batch [655/88962], Loss: 5.9735\n",
      "Epoch [1/10], Batch [656/88962], Loss: 5.7997\n",
      "Epoch [1/10], Batch [657/88962], Loss: 6.2506\n",
      "Epoch [1/10], Batch [658/88962], Loss: 6.1745\n",
      "Epoch [1/10], Batch [659/88962], Loss: 5.7808\n",
      "Epoch [1/10], Batch [660/88962], Loss: 6.2144\n",
      "Epoch [1/10], Batch [661/88962], Loss: 5.9059\n",
      "Epoch [1/10], Batch [662/88962], Loss: 5.8469\n",
      "Epoch [1/10], Batch [663/88962], Loss: 6.2858\n",
      "Epoch [1/10], Batch [664/88962], Loss: 6.1612\n",
      "Epoch [1/10], Batch [665/88962], Loss: 6.1529\n",
      "Epoch [1/10], Batch [666/88962], Loss: 5.8879\n",
      "Epoch [1/10], Batch [667/88962], Loss: 6.1601\n",
      "Epoch [1/10], Batch [668/88962], Loss: 6.4659\n",
      "Epoch [1/10], Batch [669/88962], Loss: 6.1972\n",
      "Epoch [1/10], Batch [670/88962], Loss: 6.0762\n",
      "Epoch [1/10], Batch [671/88962], Loss: 5.8082\n",
      "Epoch [1/10], Batch [672/88962], Loss: 6.3232\n",
      "Epoch [1/10], Batch [673/88962], Loss: 6.3520\n",
      "Epoch [1/10], Batch [674/88962], Loss: 5.9886\n",
      "Epoch [1/10], Batch [675/88962], Loss: 6.1862\n",
      "Epoch [1/10], Batch [676/88962], Loss: 6.1533\n",
      "Epoch [1/10], Batch [677/88962], Loss: 5.6283\n",
      "Epoch [1/10], Batch [678/88962], Loss: 6.4289\n",
      "Epoch [1/10], Batch [679/88962], Loss: 6.1309\n",
      "Epoch [1/10], Batch [680/88962], Loss: 5.9823\n",
      "Epoch [1/10], Batch [681/88962], Loss: 5.9549\n",
      "Epoch [1/10], Batch [682/88962], Loss: 6.1715\n",
      "Epoch [1/10], Batch [683/88962], Loss: 6.2721\n",
      "Epoch [1/10], Batch [684/88962], Loss: 6.2061\n",
      "Epoch [1/10], Batch [685/88962], Loss: 6.2927\n",
      "Epoch [1/10], Batch [686/88962], Loss: 5.9323\n",
      "Epoch [1/10], Batch [687/88962], Loss: 6.2364\n",
      "Epoch [1/10], Batch [688/88962], Loss: 6.1940\n",
      "Epoch [1/10], Batch [689/88962], Loss: 5.8452\n",
      "Epoch [1/10], Batch [690/88962], Loss: 5.9349\n",
      "Epoch [1/10], Batch [691/88962], Loss: 5.9115\n",
      "Epoch [1/10], Batch [692/88962], Loss: 6.0409\n",
      "Epoch [1/10], Batch [693/88962], Loss: 6.0611\n",
      "Epoch [1/10], Batch [694/88962], Loss: 6.1813\n",
      "Epoch [1/10], Batch [695/88962], Loss: 5.9372\n",
      "Epoch [1/10], Batch [696/88962], Loss: 6.0561\n",
      "Epoch [1/10], Batch [697/88962], Loss: 5.8939\n",
      "Epoch [1/10], Batch [698/88962], Loss: 6.3666\n",
      "Epoch [1/10], Batch [699/88962], Loss: 6.1659\n",
      "Epoch [1/10], Batch [700/88962], Loss: 6.1030\n",
      "Epoch [1/10], Batch [701/88962], Loss: 6.1295\n",
      "Epoch [1/10], Batch [702/88962], Loss: 6.1235\n",
      "Epoch [1/10], Batch [703/88962], Loss: 6.2065\n",
      "Epoch [1/10], Batch [704/88962], Loss: 5.7483\n",
      "Epoch [1/10], Batch [705/88962], Loss: 5.9754\n",
      "Epoch [1/10], Batch [706/88962], Loss: 5.7948\n",
      "Epoch [1/10], Batch [707/88962], Loss: 5.8273\n",
      "Epoch [1/10], Batch [708/88962], Loss: 6.1973\n",
      "Epoch [1/10], Batch [709/88962], Loss: 5.6806\n",
      "Epoch [1/10], Batch [710/88962], Loss: 6.1930\n",
      "Epoch [1/10], Batch [711/88962], Loss: 5.7820\n",
      "Epoch [1/10], Batch [712/88962], Loss: 6.0262\n",
      "Epoch [1/10], Batch [713/88962], Loss: 5.9015\n",
      "Epoch [1/10], Batch [714/88962], Loss: 6.4012\n",
      "Epoch [1/10], Batch [715/88962], Loss: 6.0998\n",
      "Epoch [1/10], Batch [716/88962], Loss: 6.0746\n",
      "Epoch [1/10], Batch [717/88962], Loss: 6.1091\n",
      "Epoch [1/10], Batch [718/88962], Loss: 6.1310\n",
      "Epoch [1/10], Batch [719/88962], Loss: 5.9917\n",
      "Epoch [1/10], Batch [720/88962], Loss: 5.8303\n",
      "Epoch [1/10], Batch [721/88962], Loss: 6.1780\n",
      "Epoch [1/10], Batch [722/88962], Loss: 5.9745\n",
      "Epoch [1/10], Batch [723/88962], Loss: 5.7025\n",
      "Epoch [1/10], Batch [724/88962], Loss: 5.8868\n",
      "Epoch [1/10], Batch [725/88962], Loss: 6.0742\n",
      "Epoch [1/10], Batch [726/88962], Loss: 6.0070\n",
      "Epoch [1/10], Batch [727/88962], Loss: 6.0519\n",
      "Epoch [1/10], Batch [728/88962], Loss: 5.8804\n",
      "Epoch [1/10], Batch [729/88962], Loss: 5.6620\n",
      "Epoch [1/10], Batch [730/88962], Loss: 6.0837\n",
      "Epoch [1/10], Batch [731/88962], Loss: 6.1114\n",
      "Epoch [1/10], Batch [732/88962], Loss: 6.0409\n",
      "Epoch [1/10], Batch [733/88962], Loss: 6.1157\n",
      "Epoch [1/10], Batch [734/88962], Loss: 5.8088\n",
      "Epoch [1/10], Batch [735/88962], Loss: 5.7141\n",
      "Epoch [1/10], Batch [736/88962], Loss: 5.7540\n",
      "Epoch [1/10], Batch [737/88962], Loss: 5.9708\n",
      "Epoch [1/10], Batch [738/88962], Loss: 5.8652\n",
      "Epoch [1/10], Batch [739/88962], Loss: 5.2817\n",
      "Epoch [1/10], Batch [740/88962], Loss: 6.0394\n",
      "Epoch [1/10], Batch [741/88962], Loss: 5.8121\n",
      "Epoch [1/10], Batch [742/88962], Loss: 5.7914\n",
      "Epoch [1/10], Batch [743/88962], Loss: 6.2966\n",
      "Epoch [1/10], Batch [744/88962], Loss: 6.0270\n",
      "Epoch [1/10], Batch [745/88962], Loss: 5.7901\n",
      "Epoch [1/10], Batch [746/88962], Loss: 5.8899\n",
      "Epoch [1/10], Batch [747/88962], Loss: 6.0192\n",
      "Epoch [1/10], Batch [748/88962], Loss: 6.2158\n",
      "Epoch [1/10], Batch [749/88962], Loss: 5.8531\n",
      "Epoch [1/10], Batch [750/88962], Loss: 5.8452\n",
      "Epoch [1/10], Batch [751/88962], Loss: 5.6448\n",
      "Epoch [1/10], Batch [752/88962], Loss: 5.7565\n",
      "Epoch [1/10], Batch [753/88962], Loss: 5.9792\n",
      "Epoch [1/10], Batch [754/88962], Loss: 5.8973\n",
      "Epoch [1/10], Batch [755/88962], Loss: 5.8962\n",
      "Epoch [1/10], Batch [756/88962], Loss: 5.9565\n",
      "Epoch [1/10], Batch [757/88962], Loss: 6.0287\n",
      "Epoch [1/10], Batch [758/88962], Loss: 5.7959\n",
      "Epoch [1/10], Batch [759/88962], Loss: 6.2833\n",
      "Epoch [1/10], Batch [760/88962], Loss: 5.5909\n",
      "Epoch [1/10], Batch [761/88962], Loss: 6.0553\n",
      "Epoch [1/10], Batch [762/88962], Loss: 5.6667\n",
      "Epoch [1/10], Batch [763/88962], Loss: 5.8202\n",
      "Epoch [1/10], Batch [764/88962], Loss: 5.9910\n",
      "Epoch [1/10], Batch [765/88962], Loss: 5.9821\n",
      "Epoch [1/10], Batch [766/88962], Loss: 5.8048\n",
      "Epoch [1/10], Batch [767/88962], Loss: 6.0807\n",
      "Epoch [1/10], Batch [768/88962], Loss: 6.0222\n",
      "Epoch [1/10], Batch [769/88962], Loss: 6.0120\n",
      "Epoch [1/10], Batch [770/88962], Loss: 5.8975\n",
      "Epoch [1/10], Batch [771/88962], Loss: 6.0991\n",
      "Epoch [1/10], Batch [772/88962], Loss: 6.0476\n",
      "Epoch [1/10], Batch [773/88962], Loss: 6.1559\n",
      "Epoch [1/10], Batch [774/88962], Loss: 6.0659\n",
      "Epoch [1/10], Batch [775/88962], Loss: 6.1867\n",
      "Epoch [1/10], Batch [776/88962], Loss: 5.7234\n",
      "Epoch [1/10], Batch [777/88962], Loss: 6.2479\n",
      "Epoch [1/10], Batch [778/88962], Loss: 5.8968\n",
      "Epoch [1/10], Batch [779/88962], Loss: 6.0279\n",
      "Epoch [1/10], Batch [780/88962], Loss: 6.0958\n",
      "Epoch [1/10], Batch [781/88962], Loss: 5.7886\n",
      "Epoch [1/10], Batch [782/88962], Loss: 5.8799\n",
      "Epoch [1/10], Batch [783/88962], Loss: 5.9567\n",
      "Epoch [1/10], Batch [784/88962], Loss: 6.0673\n",
      "Epoch [1/10], Batch [785/88962], Loss: 6.0444\n",
      "Epoch [1/10], Batch [786/88962], Loss: 5.8862\n",
      "Epoch [1/10], Batch [787/88962], Loss: 6.1325\n",
      "Epoch [1/10], Batch [788/88962], Loss: 5.9462\n",
      "Epoch [1/10], Batch [789/88962], Loss: 5.7422\n",
      "Epoch [1/10], Batch [790/88962], Loss: 6.0149\n",
      "Epoch [1/10], Batch [791/88962], Loss: 5.8936\n",
      "Epoch [1/10], Batch [792/88962], Loss: 5.7050\n",
      "Epoch [1/10], Batch [793/88962], Loss: 5.5452\n",
      "Epoch [1/10], Batch [794/88962], Loss: 6.3266\n",
      "Epoch [1/10], Batch [795/88962], Loss: 6.1947\n",
      "Epoch [1/10], Batch [796/88962], Loss: 6.1508\n",
      "Epoch [1/10], Batch [797/88962], Loss: 5.9657\n",
      "Epoch [1/10], Batch [798/88962], Loss: 5.8638\n",
      "Epoch [1/10], Batch [799/88962], Loss: 5.8922\n",
      "Epoch [1/10], Batch [800/88962], Loss: 5.7703\n",
      "Epoch [1/10], Batch [801/88962], Loss: 6.1787\n",
      "Epoch [1/10], Batch [802/88962], Loss: 5.8806\n",
      "Epoch [1/10], Batch [803/88962], Loss: 5.9038\n",
      "Epoch [1/10], Batch [804/88962], Loss: 5.8710\n",
      "Epoch [1/10], Batch [805/88962], Loss: 5.8850\n",
      "Epoch [1/10], Batch [806/88962], Loss: 5.9592\n",
      "Epoch [1/10], Batch [807/88962], Loss: 5.9558\n",
      "Epoch [1/10], Batch [808/88962], Loss: 6.1433\n",
      "Epoch [1/10], Batch [809/88962], Loss: 5.9297\n",
      "Epoch [1/10], Batch [810/88962], Loss: 6.0382\n",
      "Epoch [1/10], Batch [811/88962], Loss: 5.7174\n",
      "Epoch [1/10], Batch [812/88962], Loss: 5.5887\n",
      "Epoch [1/10], Batch [813/88962], Loss: 6.0174\n",
      "Epoch [1/10], Batch [814/88962], Loss: 5.8391\n",
      "Epoch [1/10], Batch [815/88962], Loss: 6.1932\n",
      "Epoch [1/10], Batch [816/88962], Loss: 5.8690\n",
      "Epoch [1/10], Batch [817/88962], Loss: 5.9717\n",
      "Epoch [1/10], Batch [818/88962], Loss: 6.0491\n",
      "Epoch [1/10], Batch [819/88962], Loss: 5.8177\n",
      "Epoch [1/10], Batch [820/88962], Loss: 5.7547\n",
      "Epoch [1/10], Batch [821/88962], Loss: 5.2395\n",
      "Epoch [1/10], Batch [822/88962], Loss: 5.9290\n",
      "Epoch [1/10], Batch [823/88962], Loss: 5.9159\n",
      "Epoch [1/10], Batch [824/88962], Loss: 6.0987\n",
      "Epoch [1/10], Batch [825/88962], Loss: 5.6442\n",
      "Epoch [1/10], Batch [826/88962], Loss: 5.7969\n",
      "Epoch [1/10], Batch [827/88962], Loss: 5.7216\n",
      "Epoch [1/10], Batch [828/88962], Loss: 5.6487\n",
      "Epoch [1/10], Batch [829/88962], Loss: 6.0148\n",
      "Epoch [1/10], Batch [830/88962], Loss: 5.9914\n",
      "Epoch [1/10], Batch [831/88962], Loss: 5.9393\n",
      "Epoch [1/10], Batch [832/88962], Loss: 5.8775\n",
      "Epoch [1/10], Batch [833/88962], Loss: 6.3342\n",
      "Epoch [1/10], Batch [834/88962], Loss: 5.6795\n",
      "Epoch [1/10], Batch [835/88962], Loss: 5.9195\n",
      "Epoch [1/10], Batch [836/88962], Loss: 5.8070\n",
      "Epoch [1/10], Batch [837/88962], Loss: 5.9233\n",
      "Epoch [1/10], Batch [838/88962], Loss: 5.9763\n",
      "Epoch [1/10], Batch [839/88962], Loss: 5.9332\n",
      "Epoch [1/10], Batch [840/88962], Loss: 5.9969\n",
      "Epoch [1/10], Batch [841/88962], Loss: 5.7079\n",
      "Epoch [1/10], Batch [842/88962], Loss: 5.8477\n",
      "Epoch [1/10], Batch [843/88962], Loss: 6.0174\n",
      "Epoch [1/10], Batch [844/88962], Loss: 5.6562\n",
      "Epoch [1/10], Batch [845/88962], Loss: 5.9783\n",
      "Epoch [1/10], Batch [846/88962], Loss: 5.7515\n",
      "Epoch [1/10], Batch [847/88962], Loss: 6.3037\n",
      "Epoch [1/10], Batch [848/88962], Loss: 5.6081\n",
      "Epoch [1/10], Batch [849/88962], Loss: 5.9093\n",
      "Epoch [1/10], Batch [850/88962], Loss: 6.1570\n",
      "Epoch [1/10], Batch [851/88962], Loss: 5.7410\n",
      "Epoch [1/10], Batch [852/88962], Loss: 5.8924\n",
      "Epoch [1/10], Batch [853/88962], Loss: 5.5975\n",
      "Epoch [1/10], Batch [854/88962], Loss: 5.8632\n",
      "Epoch [1/10], Batch [855/88962], Loss: 6.0446\n",
      "Epoch [1/10], Batch [856/88962], Loss: 5.9234\n",
      "Epoch [1/10], Batch [857/88962], Loss: 5.5408\n",
      "Epoch [1/10], Batch [858/88962], Loss: 6.1118\n",
      "Epoch [1/10], Batch [859/88962], Loss: 5.8897\n",
      "Epoch [1/10], Batch [860/88962], Loss: 5.9630\n",
      "Epoch [1/10], Batch [861/88962], Loss: 5.9009\n",
      "Epoch [1/10], Batch [862/88962], Loss: 5.8735\n",
      "Epoch [1/10], Batch [863/88962], Loss: 6.1430\n",
      "Epoch [1/10], Batch [864/88962], Loss: 6.1541\n",
      "Epoch [1/10], Batch [865/88962], Loss: 6.0287\n",
      "Epoch [1/10], Batch [866/88962], Loss: 5.8094\n",
      "Epoch [1/10], Batch [867/88962], Loss: 6.0146\n",
      "Epoch [1/10], Batch [868/88962], Loss: 5.9347\n",
      "Epoch [1/10], Batch [869/88962], Loss: 6.0970\n",
      "Epoch [1/10], Batch [870/88962], Loss: 5.8082\n",
      "Epoch [1/10], Batch [871/88962], Loss: 5.7534\n",
      "Epoch [1/10], Batch [872/88962], Loss: 5.9873\n",
      "Epoch [1/10], Batch [873/88962], Loss: 5.8100\n",
      "Epoch [1/10], Batch [874/88962], Loss: 6.0754\n",
      "Epoch [1/10], Batch [875/88962], Loss: 5.8764\n",
      "Epoch [1/10], Batch [876/88962], Loss: 5.9540\n",
      "Epoch [1/10], Batch [877/88962], Loss: 6.1094\n",
      "Epoch [1/10], Batch [878/88962], Loss: 5.8776\n",
      "Epoch [1/10], Batch [879/88962], Loss: 5.9271\n",
      "Epoch [1/10], Batch [880/88962], Loss: 5.5702\n",
      "Epoch [1/10], Batch [881/88962], Loss: 5.8867\n",
      "Epoch [1/10], Batch [882/88962], Loss: 5.9231\n",
      "Epoch [1/10], Batch [883/88962], Loss: 5.8524\n",
      "Epoch [1/10], Batch [884/88962], Loss: 5.9432\n",
      "Epoch [1/10], Batch [885/88962], Loss: 5.9638\n",
      "Epoch [1/10], Batch [886/88962], Loss: 5.7924\n",
      "Epoch [1/10], Batch [887/88962], Loss: 5.7228\n",
      "Epoch [1/10], Batch [888/88962], Loss: 5.8721\n",
      "Epoch [1/10], Batch [889/88962], Loss: 5.7844\n",
      "Epoch [1/10], Batch [890/88962], Loss: 5.6204\n",
      "Epoch [1/10], Batch [891/88962], Loss: 6.3474\n",
      "Epoch [1/10], Batch [892/88962], Loss: 5.8412\n",
      "Epoch [1/10], Batch [893/88962], Loss: 5.6657\n",
      "Epoch [1/10], Batch [894/88962], Loss: 5.9998\n",
      "Epoch [1/10], Batch [895/88962], Loss: 5.8738\n",
      "Epoch [1/10], Batch [896/88962], Loss: 5.6596\n",
      "Epoch [1/10], Batch [897/88962], Loss: 5.6724\n",
      "Epoch [1/10], Batch [898/88962], Loss: 6.0418\n",
      "Epoch [1/10], Batch [899/88962], Loss: 5.8353\n",
      "Epoch [1/10], Batch [900/88962], Loss: 5.5383\n",
      "Epoch [1/10], Batch [901/88962], Loss: 5.7546\n",
      "Epoch [1/10], Batch [902/88962], Loss: 5.7197\n",
      "Epoch [1/10], Batch [903/88962], Loss: 6.0267\n",
      "Epoch [1/10], Batch [904/88962], Loss: 6.0476\n",
      "Epoch [1/10], Batch [905/88962], Loss: 5.8107\n",
      "Epoch [1/10], Batch [906/88962], Loss: 5.6479\n",
      "Epoch [1/10], Batch [907/88962], Loss: 5.7361\n",
      "Epoch [1/10], Batch [908/88962], Loss: 5.5219\n",
      "Epoch [1/10], Batch [909/88962], Loss: 6.1002\n",
      "Epoch [1/10], Batch [910/88962], Loss: 5.8107\n",
      "Epoch [1/10], Batch [911/88962], Loss: 5.7767\n",
      "Epoch [1/10], Batch [912/88962], Loss: 5.8179\n",
      "Epoch [1/10], Batch [913/88962], Loss: 5.9201\n",
      "Epoch [1/10], Batch [914/88962], Loss: 6.0035\n",
      "Epoch [1/10], Batch [915/88962], Loss: 5.5324\n",
      "Epoch [1/10], Batch [916/88962], Loss: 5.8054\n",
      "Epoch [1/10], Batch [917/88962], Loss: 5.9248\n",
      "Epoch [1/10], Batch [918/88962], Loss: 5.5667\n",
      "Epoch [1/10], Batch [919/88962], Loss: 5.8380\n",
      "Epoch [1/10], Batch [920/88962], Loss: 5.6864\n",
      "Epoch [1/10], Batch [921/88962], Loss: 5.7872\n",
      "Epoch [1/10], Batch [922/88962], Loss: 5.7217\n",
      "Epoch [1/10], Batch [923/88962], Loss: 5.8352\n",
      "Epoch [1/10], Batch [924/88962], Loss: 6.1952\n",
      "Epoch [1/10], Batch [925/88962], Loss: 6.1305\n",
      "Epoch [1/10], Batch [926/88962], Loss: 5.3360\n",
      "Epoch [1/10], Batch [927/88962], Loss: 6.0743\n",
      "Epoch [1/10], Batch [928/88962], Loss: 6.3553\n",
      "Epoch [1/10], Batch [929/88962], Loss: 5.7968\n",
      "Epoch [1/10], Batch [930/88962], Loss: 5.7914\n",
      "Epoch [1/10], Batch [931/88962], Loss: 5.8345\n",
      "Epoch [1/10], Batch [932/88962], Loss: 6.1768\n",
      "Epoch [1/10], Batch [933/88962], Loss: 5.7336\n",
      "Epoch [1/10], Batch [934/88962], Loss: 6.1676\n",
      "Epoch [1/10], Batch [935/88962], Loss: 6.0684\n",
      "Epoch [1/10], Batch [936/88962], Loss: 5.9914\n",
      "Epoch [1/10], Batch [937/88962], Loss: 5.7501\n",
      "Epoch [1/10], Batch [938/88962], Loss: 5.8054\n",
      "Epoch [1/10], Batch [939/88962], Loss: 6.1431\n",
      "Epoch [1/10], Batch [940/88962], Loss: 5.8845\n",
      "Epoch [1/10], Batch [941/88962], Loss: 5.6863\n",
      "Epoch [1/10], Batch [942/88962], Loss: 6.0132\n",
      "Epoch [1/10], Batch [943/88962], Loss: 5.8618\n",
      "Epoch [1/10], Batch [944/88962], Loss: 5.9168\n",
      "Epoch [1/10], Batch [945/88962], Loss: 5.6869\n",
      "Epoch [1/10], Batch [946/88962], Loss: 6.2519\n",
      "Epoch [1/10], Batch [947/88962], Loss: 5.7223\n",
      "Epoch [1/10], Batch [948/88962], Loss: 5.6928\n",
      "Epoch [1/10], Batch [949/88962], Loss: 5.5855\n",
      "Epoch [1/10], Batch [950/88962], Loss: 5.9994\n",
      "Epoch [1/10], Batch [951/88962], Loss: 5.9322\n",
      "Epoch [1/10], Batch [952/88962], Loss: 5.6571\n",
      "Epoch [1/10], Batch [953/88962], Loss: 6.0820\n",
      "Epoch [1/10], Batch [954/88962], Loss: 5.6211\n",
      "Epoch [1/10], Batch [955/88962], Loss: 6.1415\n",
      "Epoch [1/10], Batch [956/88962], Loss: 5.8897\n",
      "Epoch [1/10], Batch [957/88962], Loss: 5.7416\n",
      "Epoch [1/10], Batch [958/88962], Loss: 6.0999\n",
      "Epoch [1/10], Batch [959/88962], Loss: 5.8083\n",
      "Epoch [1/10], Batch [960/88962], Loss: 5.7699\n",
      "Epoch [1/10], Batch [961/88962], Loss: 5.6855\n",
      "Epoch [1/10], Batch [962/88962], Loss: 6.0236\n",
      "Epoch [1/10], Batch [963/88962], Loss: 5.9219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thehe\\AppData\\Local\\Temp\\ipykernel_28644\\487771388.py:9: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  df = pd.read_csv(\"D:\\projects\\Seq2Seq Model\\DataSet\\EN-DE\\EN-DE.txt\", sep = \"\\t\", header= None)[[0,1]].rename(columns = {0:SL, 1:TL})\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m i \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m----> 6\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43mbatch_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_data\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch_labels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\python_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mC:\\python_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    671\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    672\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 673\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    675\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mC:\\python_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[7], line 23\u001B[0m, in \u001B[0;36menglish_german_translation.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     20\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[idx]\n\u001B[0;32m     21\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget[idx]\n\u001B[1;32m---> 23\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstring_to_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgerman_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstring_to_tokens(y,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menglish_dict,\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x,y\n",
      "Cell \u001B[1;32mIn[7], line 29\u001B[0m, in \u001B[0;36menglish_german_translation.string_to_tokens\u001B[1;34m(self, string, dict, gate)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstring_to_tokens\u001B[39m(\u001B[38;5;28mself\u001B[39m,string,\u001B[38;5;28mdict\u001B[39m,gate):\n\u001B[1;32m---> 29\u001B[0m     string \u001B[38;5;241m=\u001B[39m \u001B[43mstring\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlower\u001B[49m()\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;66;03m# removes punctuations and split into words\u001B[39;00m\n\u001B[0;32m     31\u001B[0m     string \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39mfindall(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m, string)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T09:34:09.164787Z",
     "start_time": "2024-09-05T09:34:04.592460Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c34d285face9c2b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "76b61f5d7ae2c0f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
